\addvspace {10\p@ }
\contentsline {table}{\numberline {1.1}{\ignorespaces Number of layers and proposed years of various networks.}}{2}
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces Number of parameters in LeNet5-431K.}}{12}
\contentsline {table}{\numberline {2.2}{\ignorespaces Number of parameters in CifarNet.}}{13}
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Number of parameters of pruned LeNet5-431K using \textit {Determinisitc pruning}, and the pruning includes weights and biases.}}{17}
\contentsline {table}{\numberline {3.2}{\ignorespaces Number of parameters of pruned \textit {Cifarnet} using \textit {Determinisitc pruning}, and the pruning includes weights and biases.}}{17}
\contentsline {table}{\numberline {3.3}{\ignorespaces Number of parameters of pruned LeNet5-431K using \textit {Deterministic pruning}. Prune(a) is pruning both weights and biases, Prune(b) is pruning weights only.}}{18}
\contentsline {table}{\numberline {3.4}{\ignorespaces Number of parameters of pruned CifarNet using \textit {Deterministic pruning}. Prune(a) is pruning both weights and biases, Prune(b) is pruning weights only.}}{18}
\contentsline {table}{\numberline {3.5}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit {Deterministic pruning} with both weights and biases, Prune(b) is \textit {Deterministic pruning} with weights only and Prune(c) is \textit {Dynamic network surgery}.}}{21}
\contentsline {table}{\numberline {3.6}{\ignorespaces Number of parameters of pruned CifarNet. Prune(a) is \textit {Deterministic pruning} with both weights and biases, Prune(b) is \textit {Deterministic pruning} with weights only and Prune(c) is \textit {Dynamic network surgery}.}}{21}
\contentsline {table}{\numberline {3.7}{\ignorespaces \textit {LeNet5} pruning summary, CR is the compression rate, ER is the error rate. (\textit {Han}) is \textit {Deterministic pruning} used by \textit {Han et al.}. (\textit {Guo}) is the original \textit {Dynamic network surgery} implemented by \textit {Guo et al.}. (a), (b), (c), (d) are my implementations of various methods. (a) is \textit {Deterministic pruning} with weights and biases, (b) is \textit {Deterministic pruning} with weights only, (c) is \textit {Dynamic network surgery}, (d) is also \textit {Dynamic network surgery} but with the same error rate as (\textit {Guo}).}}{22}
\contentsline {table}{\numberline {3.8}{\ignorespaces \textit {CifarNet} pruning summary, CR is the compression rate, ER is the error rate. (a) is \textit {Deterministic pruning} with weights and biases, (b) is \textit {Deterministic pruning} with weights only, (c) is \textit {Dynamic network surgery}.}}{23}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit {Gradient profiling}, Prune(b) is \textit {Dynamic Network Surgery}.}}{29}
\contentsline {table}{\numberline {4.2}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit {Regularization aided pruning} with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-4}$ and $\lambda _2 = 1e^{-7}$. Prune(b) is \textit {Dynamic network surgery}.}}{31}
\contentsline {table}{\numberline {4.3}{\ignorespaces Number of parameters of pruned CifarNet. Prune(a) is \textit {Regularization aided pruning} with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-5}$ and $\lambda _2 = 1e^{-5}$. Prune(b) is \textit {Dynamic Network Surgery}.}}{32}
\contentsline {table}{\numberline {4.4}{\ignorespaces LeNet5 Pruning Summary, CR is the compression rate, ER is the error rate. (a) is \textit {Deterministic pruning} with weights and biases, (b) is \textit {Deterministic pruning} with weights only, (c) is \textit {Dynamic network surgery}, (d) is \textit {Gradient profiling} and (e) is \textit {Regularization aided pruning}.}}{32}
\contentsline {table}{\numberline {4.5}{\ignorespaces \textit {CifarNet} Pruning Summary, CR is the compression rate, ER is the error rate. (a) is \textit {Deterministic pruning} with weights and biases, (b) is \textit {Deterministic pruning} with weights only, (c) is \textit {Dynamic network surgery} and (d) is \textit {Regularization aided pruning}.}}{33}
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces \textit {Weights sharing} summary for \textit {LeNet5} and \textit {CifarNet}.}}{36}
\contentsline {table}{\numberline {5.2}{\ignorespaces Fixed-point quantization summary for \textit {LeNet5} and \textit {CifarNet}.}}{38}
\contentsline {table}{\numberline {5.3}{\ignorespaces \textit {Dynamic fixed-point quantization} summary for \textit {LeNet5} and \textit {CifarNet}.}}{40}
\addvspace {10\p@ }
\contentsline {table}{\numberline {6.1}{\ignorespaces \textit {Customized floating-point} quantization summary for \textit {LeNet5} and \textit {CifarNet}, with 2-bit exponent and various mantissa widths.}}{44}
\contentsline {table}{\numberline {6.2}{\ignorespaces \textit {Customized floating-point} quantization summary for \textit {LeNet5}, with 1-bit sign, 3-bit mantissa and various exponent widths.}}{44}
\contentsline {table}{\numberline {6.3}{\ignorespaces Quantization summary on a pruned \textit {LeNet5} model. \textit {Customized floating-point} has 1-bit sign, 1-bit exponent and the rest are mantissa bits. \textit {Centralized customized floating-point} has 1-bit sign, 1-bit central, 1-bit exponent and the rest are mantissa bits. \textit {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{50}
\contentsline {table}{\numberline {6.4}{\ignorespaces Quantization summary on a pruned \textit {CifarNet} model. \textit {Customized floating-point} has 1-bit sign, 2-bit exponent and the rest are mantissa bits. \textit {Centralized customized floating-point} has 1-bit sign, 1-bit central, 2-bit exponent and the rest are mantissa bits. \textit {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{51}
\contentsline {table}{\numberline {6.5}{\ignorespaces Quantization summary (bit-width, error rate) on unpruned \textit {LeNet5} and \textit {CifarNet}. \textit {DFP} is \textit {Dynamic fixed-point}, \textit {CFP} is \textit {Customized floating-point} and (\textit {Gysel}) is \textit {Gysel et al.}'s implemenetation of \textit {Dynamic fixed-point} \cite {Gysel}.}}{52}
\contentsline {table}{\numberline {6.6}{\ignorespaces Quantization summary (bit-width, error rate) on pruned \textit {LeNet5} and \textit {CifarNet}. \textit {CFP} is \textit {Customized floating-point} \textit {CCFP} is \textit {Centralized customized floating-point} and \textit {CDFP} is \textit {Centralized dynmaic fixed-point}. }}{52}
\addvspace {10\p@ }
\contentsline {table}{\numberline {7.1}{\ignorespaces \textit {LeNet5} compression summary, Pruning and Quantization, CR is the compression rate, ER is the error rate. P represnets pruning and Q reporesents quantization.}}{55}
\contentsline {table}{\numberline {7.2}{\ignorespaces \textit {CifarNet} compression summary, Pruning and Quantization, CR is the compression rate, ER is the error rate. P represnets pruning and Q reporesents quantization.}}{56}
\addvspace {10\p@ }
