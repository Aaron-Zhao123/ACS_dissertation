\relax 
\citation{deng2009imagenet}
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Guo}
\citation{Han15}
\citation{Han15}
\citation{Gysel}
\citation{Krizhevsky}
\citation{Lecun1998gradient}
\citation{Szegedy}
\citation{Lecun1998gradient}
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Coates}
\citation{deng2009imagenet}
\citation{Krizhevsky}
\citation{DBLP:journals/corr/SimonyanZ14a}
\citation{DBLP:journals/corr/IoffeS15}
\citation{DBLP:journals/corr/SzegedyVISW15}
\citation{DBLP:journals/corr/HeZRS15}
\citation{deng2009imagenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{deng2009imagenet}
\citation{deng2009imagenet}
\citation{Mnih}
\citation{Mnih2016}
\citation{Han15}
\citation{DBLP:journals/corr/SimonyanZ14a}
\citation{Tien}
\citation{Tien}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Number of layers and proposed years of various networks.}}{2}}
\newlabel{tab:networkmodels}{{1.1}{2}}
\newlabel{tab:networkmodels@cref}{{[table][1][1]1.1}{2}}
\citation{chen2014dadiannao}
\citation{chen2014diannao}
\citation{han2016eie}
\citation{han2016ese}
\citation{zhang2015optimizing}
\citation{Nurvitadhi:2017:FBG:3020078.3021740}
\citation{han2016eie}
\citation{chen2017eyeriss}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Popular networks targeting on \textit  {ImageNet ILSVRC 2012} dataset \cite  {deng2009imagenet}.}}{3}}
\newlabel{fig:network_size}{{1.1}{3}}
\newlabel{fig:network_size@cref}{{[figure][1][1]1.1}{3}}
\citation{Han15}
\citation{Guo}
\citation{Krizhevsky}
\citation{Goodfellow-et-al-2016}
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Krizhevsky}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Networks}{5}}
\citation{Mladenic}
\citation{Krizhevsky}
\citation{hecht1988theory}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces AlexNet Network Architecture \cite  {Krizhevsky}.}}{6}}
\newlabel{fig:alexnet}{{2.1}{6}}
\newlabel{fig:alexnet@cref}{{[figure][1][2]2.1}{6}}
\citation{Lecun1998gradient}
\citation{Krizhevsky}
\citation{Coates}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Fully connected layer followed by an activation function \cite  {Krizhevsky}.}}{7}}
\newlabel{fig:neuron}{{2.2}{7}}
\newlabel{fig:neuron@cref}{{[figure][2][2]2.2}{7}}
\newlabel{equ:backprop}{{2.1}{7}}
\newlabel{equ:backprop@cref}{{[equation][1][2]2.1}{7}}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/SzegedyVISW15}
\citation{Lecun1998gradient}
\citation{Hassibi}
\citation{Srinivas2015}
\citation{mao2017exploring}
\citation{Lecun1998gradient}
\citation{Hassibi}
\citation{Han15}
\citation{Guo}
\citation{DBLP:journals/corr/LiKDSG16}
\citation{DBLP:journals/corr/WenWWCL16}
\citation{DBLP:journals/corr/LebedevL15}
\citation{DBLP:journals/corr/LiKDSG16}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{8}}
\citation{Srinivas2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Taxonomy of compression techniques.}}{9}}
\newlabel{fig:tax}{{2.3}{9}}
\newlabel{fig:tax@cref}{{[figure][3][2]2.3}{9}}
\citation{Han15}
\citation{Han15}
\citation{chen2015compressing}
\citation{moons2016energy}
\citation{Hubara}
\citation{Courbariaux}
\citation{li2016ternary}
\citation{tensorflow}
\citation{lecun1998mnist}
\citation{krizhevsky2014cifar}
\citation{deng2009imagenet}
\citation{grother1995nist}
\citation{lecun1998mnist}
\citation{lecun2015lenet}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Experiment Setups, Datasets and Trained Networks}{11}}
\citation{torralba200880}
\citation{krizhevsky2014cifar}
\citation{krizhevsky2009learning}
\citation{jia2014caffe}
\citation{Han15}
\citation{Guo}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Number of parameters in LeNet5-431K.}}{12}}
\newlabel{tab:LeNetparam}{{2.1}{12}}
\newlabel{tab:LeNetparam@cref}{{[table][1][2]2.1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces LeNet5 Network Architecture}}{12}}
\newlabel{fig:lenet5_arch}{{2.4}{12}}
\newlabel{fig:lenet5_arch@cref}{{[figure][4][2]2.4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces CifarNet Network Architecture}}{13}}
\newlabel{fig:Cifarnetparam}{{2.5}{13}}
\newlabel{fig:Cifarnetparam@cref}{{[figure][5][2]2.5}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Number of parameters in CifarNet.}}{13}}
\newlabel{tab:CifarNetparam}{{2.2}{13}}
\newlabel{tab:CifarNetparam@cref}{{[table][2][2]2.2}{13}}
\citation{mao2017exploring}
\citation{Han}
\citation{Guo}
\citation{Thimm}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Pruning}{14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:dprune}{{3}{14}}
\newlabel{sec:dprune@cref}{{[chapter][3][]3}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Deterministic Pruning}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Pruning Both Weights and Biases}{14}}
\newlabel{sec:prune_without_biase}{{3.1.1}{14}}
\newlabel{sec:prune_without_biase@cref}{{[subsection][1][3,1]3.1.1}{14}}
\citation{Han}
\citation{Guo}
\newlabel{equ:hfunc}{{3.1}{15}}
\newlabel{equ:hfunc@cref}{{[equation][1][3]3.1}{15}}
\newlabel{equ:tn}{{3.2}{15}}
\newlabel{equ:tn@cref}{{[equation][2][3]3.2}{15}}
\citation{Guo}
\newlabel{equ:minfunc}{{3.3}{16}}
\newlabel{equ:minfunc@cref}{{[equation][3][3]3.3}{16}}
\newlabel{equ:trainfunc}{{3.4}{16}}
\newlabel{equ:trainfunc@cref}{{[equation][4][3]3.4}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Distribution of weights in the first fully-connected layer of pruned and unpruned LeNet5}}{17}}
\newlabel{fig:dist_weights}{{3.1}{17}}
\newlabel{fig:dist_weights@cref}{{[figure][1][3]3.1}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Number of parameters of pruned LeNet5-431K using \textit  {Determinisitc pruning}, and the pruning includes weights and biases.}}{17}}
\newlabel{tab:LeNetPrune}{{3.1}{17}}
\newlabel{tab:LeNetPrune@cref}{{[table][1][3]3.1}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Number of parameters of pruned \textit  {Cifarnet} using \textit  {Determinisitc pruning}, and the pruning includes weights and biases.}}{17}}
\newlabel{tab:CifarNetPrune}{{3.2}{17}}
\newlabel{tab:CifarNetPrune@cref}{{[table][2][3]3.2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Pruning Weights Only}{18}}
\newlabel{equ:lossmin2}{{3.5}{18}}
\newlabel{equ:lossmin2@cref}{{[equation][5][3]3.5}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Number of parameters of pruned LeNet5-431K using \textit  {Deterministic pruning}. Prune(a) is pruning both weights and biases, Prune(b) is pruning weights only.}}{18}}
\newlabel{tab:LeNetPrune2}{{3.3}{18}}
\newlabel{tab:LeNetPrune2@cref}{{[table][3][3]3.3}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Number of parameters of pruned CifarNet using \textit  {Deterministic pruning}. Prune(a) is pruning both weights and biases, Prune(b) is pruning weights only.}}{18}}
\newlabel{tab:CifarNetPrune2}{{3.4}{18}}
\newlabel{tab:CifarNetPrune2@cref}{{[table][4][3]3.4}{18}}
\citation{Guo}
\citation{Guo}
\citation{Guo}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dynamic Network Surgery}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Mechanism of \textit  {Dynamic network surgery} \cite  {Guo}.}}{19}}
\newlabel{fig:dynamic_network_surgery}{{3.2}{19}}
\newlabel{fig:dynamic_network_surgery@cref}{{[figure][2][3]3.2}{19}}
\newlabel{equ:hfunc_ds}{{3.6}{20}}
\newlabel{equ:hfunc_ds@cref}{{[equation][6][3]3.6}{20}}
\newlabel{equ:hfunc_ds}{{3.7}{20}}
\newlabel{equ:hfunc_ds@cref}{{[equation][7][3]3.7}{20}}
\citation{Han15}
\citation{Guo}
\citation{Han15}
\citation{Guo}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit  {Deterministic pruning} with both weights and biases, Prune(b) is \textit  {Deterministic pruning} with weights only and Prune(c) is \textit  {Dynamic network surgery}.}}{21}}
\newlabel{tab:LeNetPrune3}{{3.5}{21}}
\newlabel{tab:LeNetPrune3@cref}{{[table][5][3]3.5}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Number of parameters of pruned CifarNet. Prune(a) is \textit  {Deterministic pruning} with both weights and biases, Prune(b) is \textit  {Deterministic pruning} with weights only and Prune(c) is \textit  {Dynamic network surgery}.}}{21}}
\newlabel{tab:CifarNetPrune3}{{3.6}{21}}
\newlabel{tab:CifarNetPrune3@cref}{{[table][6][3]3.6}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Comparison to Existing Works}{21}}
\newlabel{sec:pruning_ext_comp}{{3.3}{21}}
\newlabel{sec:pruning_ext_comp@cref}{{[section][3][3]3.3}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces \textit  {LeNet5} pruning summary, CR is the compression rate, ER is the error rate. (\textit  {Han}) is \textit  {Deterministic pruning} used by \textit  {Han et al.}. (\textit  {Guo}) is the original \textit  {Dynamic network surgery} implemented by \textit  {Guo et al.}. (a), (b), (c), (d) are my implementations of various methods. (a) is \textit  {Deterministic pruning} with weights and biases, (b) is \textit  {Deterministic pruning} with weights only, (c) is \textit  {Dynamic network surgery}, (d) is also \textit  {Dynamic network surgery} but with the same error rate as (\textit  {Guo}).}}{22}}
\newlabel{fig:prune_org_summary}{{3.7}{22}}
\newlabel{fig:prune_org_summary@cref}{{[table][7][3]3.7}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces \textit  {CifarNet} pruning summary, CR is the compression rate, ER is the error rate. (a) is \textit  {Deterministic pruning} with weights and biases, (b) is \textit  {Deterministic pruning} with weights only, (c) is \textit  {Dynamic network surgery}.}}{23}}
\newlabel{fig:cifar_prune_org_summary}{{3.8}{23}}
\newlabel{fig:cifar_prune_org_summary@cref}{{[table][8][3]3.8}{23}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Optimized Pruning Strategies}{24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Gradient Profiling}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Method Description}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Pruning with \textit  {Gradient profiling}. The top row is identical to \textit  {Dynamic network surgery}, the second row is the three phases of \textit  {Gradient profiling}}}{25}}
\newlabel{fig:gp_mech}{{4.1}{25}}
\newlabel{fig:gp_mech@cref}{{[figure][1][4]4.1}{25}}
\citation{molchanov2016pruning}
\newlabel{equ:gmfunc}{{4.1}{26}}
\newlabel{equ:gmfunc@cref}{{[equation][1][4]4.1}{26}}
\newlabel{equ:minfuncgp}{{4.2}{26}}
\newlabel{equ:minfuncgp@cref}{{[equation][2][4]4.2}{26}}
\newlabel{equ:trainfuncgp}{{4.3}{26}}
\newlabel{equ:trainfuncgp@cref}{{[equation][3][4]4.3}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Gradient Profiling and Retraining}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textit  {Gradient profiling} and \textit  {Dynamic network surgery} without retraining. The figure shows how various compressions affect test accuracies.}}{28}}
\newlabel{fig:gpvsds_noretrain}{{4.2}{28}}
\newlabel{fig:gpvsds_noretrain@cref}{{[figure][2][4]4.2}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textit  {Gradient profiling} and \textit  {Dynamic network surgery} with 10 epochs retraining. The figure shows how various compressed sizes affect test accuracies.}}{28}}
\newlabel{fig:gpvsds_10retrain}{{4.3}{28}}
\newlabel{fig:gpvsds_10retrain@cref}{{[figure][3][4]4.3}{28}}
\citation{Han}
\citation{Guo}
\citation{nie2010efficient}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit  {Gradient profiling}, Prune(b) is \textit  {Dynamic Network Surgery}.}}{29}}
\newlabel{tab:LeNetPrunegp}{{4.1}{29}}
\newlabel{tab:LeNetPrunegp@cref}{{[table][1][4]4.1}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Regularization Aided Pruning}{29}}
\citation{nie2010efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}$l_1$ and $l_2$ Norms}{30}}
\newlabel{equ:norms}{{4.5}{30}}
\newlabel{equ:norms@cref}{{[equation][5][4]4.5}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Effects of $l1$ and $l2$ norms with different hyperparameters.}}{30}}
\newlabel{fig:l1l2norm}{{4.4}{30}}
\newlabel{fig:l1l2norm@cref}{{[figure][4][4]4.4}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Number of parameters of pruned LeNet5-431K. Prune(a) is \textit  {Regularization aided pruning} with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-4}$ and $\lambda _2 = 1e^{-7}$. Prune(b) is \textit  {Dynamic network surgery}.}}{31}}
\newlabel{tab:LeNetprunel1l2}{{4.2}{31}}
\newlabel{tab:LeNetprunel1l2@cref}{{[table][2][4]4.2}{31}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Number of parameters of pruned CifarNet. Prune(a) is \textit  {Regularization aided pruning} with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-5}$ and $\lambda _2 = 1e^{-5}$. Prune(b) is \textit  {Dynamic Network Surgery}.}}{32}}
\newlabel{tab:CifarNetPrunel1l2}{{4.3}{32}}
\newlabel{tab:CifarNetPrunel1l2@cref}{{[table][3][4]4.3}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Summary of Pruning Methods}{32}}
\newlabel{sec:pruning_sum}{{4.3}{32}}
\newlabel{sec:pruning_sum@cref}{{[section][3][4]4.3}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces LeNet5 Pruning Summary, CR is the compression rate, ER is the error rate. (a) is \textit  {Deterministic pruning} with weights and biases, (b) is \textit  {Deterministic pruning} with weights only, (c) is \textit  {Dynamic network surgery}, (d) is \textit  {Gradient profiling} and (e) is \textit  {Regularization aided pruning}.}}{32}}
\newlabel{fig:prune_new_summary}{{4.4}{32}}
\newlabel{fig:prune_new_summary@cref}{{[table][4][4]4.4}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces \textit  {CifarNet} Pruning Summary, CR is the compression rate, ER is the error rate. (a) is \textit  {Deterministic pruning} with weights and biases, (b) is \textit  {Deterministic pruning} with weights only, (c) is \textit  {Dynamic network surgery} and (d) is \textit  {Regularization aided pruning}.}}{33}}
\newlabel{fig:cifar_prune_new_summary}{{4.5}{33}}
\newlabel{fig:cifar_prune_new_summary@cref}{{[table][5][4]4.5}{33}}
\citation{mellempudi2017mixed}
\citation{Han15}
\citation{chen2015compressing}
\citation{Han15}
\citation{Han15}
\citation{kanungo2002efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Quantization}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Weights Sharing}{34}}
\citation{Han15}
\citation{Han15}
\newlabel{equ:weightsshare_cr}{{5.1}{35}}
\newlabel{equ:weightsshare_cr@cref}{{[equation][1][5]5.1}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Weights sharing: training (bottom) and inference (top) \cite  {Han15}.}}{35}}
\newlabel{fig:weights_share}{{5.1}{35}}
\newlabel{fig:weights_share@cref}{{[figure][1][5]5.1}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \textit  {Weights sharing} summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{36}}
\newlabel{tab:ws_sum}{{5.1}{36}}
\newlabel{tab:ws_sum@cref}{{[table][1][5]5.1}{36}}
\citation{ercegovac2004digital}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fixed-point Quantization}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Number representation system for fixed-point quantization with 1-bit sign and N-bit fraction.}}{37}}
\newlabel{fig:number_rep_4bit}{{5.2}{37}}
\newlabel{fig:number_rep_4bit@cref}{{[figure][2][5]5.2}{37}}
\newlabel{equ:d2fp}{{5.2}{37}}
\newlabel{equ:d2fp@cref}{{[equation][2][5]5.2}{37}}
\citation{williamson1991dynamically}
\citation{courbariaux2014training}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Fixed-point quantization summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{38}}
\newlabel{tab:fp_sum}{{5.2}{38}}
\newlabel{tab:fp_sum@cref}{{[table][2][5]5.2}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Dynamic Fixed-point Quantization}{38}}
\newlabel{sec:dfq}{{5.3}{38}}
\newlabel{sec:dfq@cref}{{[section][3][5]5.3}{38}}
\citation{courbariaux2014training}
\citation{Gysel}
\citation{courbariaux2014training}
\citation{Gysel}
\newlabel{equ:d2ddfp}{{5.3}{39}}
\newlabel{equ:d2ddfp@cref}{{[equation][3][5]5.3}{39}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Scaling Factor Update}}{39}}
\newlabel{alg:scaling_factor}{{1}{39}}
\newlabel{alg:scaling_factor@cref}{{[algorithm][1][]1}{39}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces \textit  {Dynamic fixed-point quantization} summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{40}}
\newlabel{tab:dfp_sum}{{5.3}{40}}
\newlabel{tab:dfp_sum@cref}{{[table][3][5]5.3}{40}}
\citation{courbariaux2014training}
\citation{Gysel}
\citation{ercegovac2004digital}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Optimized Quantization Strategies}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Customized Floating-point Quantization}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Number representation system for customized floating-point quantization with e-bit exponent and m-bit mantissa.}}{43}}
\newlabel{number_rep_dfp}{{6.1}{43}}
\newlabel{number_rep_dfp@cref}{{[figure][1][6]6.1}{43}}
\newlabel{equ:d2dfp2}{{6.1}{43}}
\newlabel{equ:d2dfp2@cref}{{[equation][1][6]6.1}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \textit  {Customized floating-point} quantization summary for \textit  {LeNet5} and \textit  {CifarNet}, with 2-bit exponent and various mantissa widths.}}{44}}
\newlabel{tab:cfp_sum}{{6.1}{44}}
\newlabel{tab:cfp_sum@cref}{{[table][1][6]6.1}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \textit  {Customized floating-point} quantization summary for \textit  {LeNet5}, with 1-bit sign, 3-bit mantissa and various exponent widths.}}{44}}
\newlabel{tab:cfp_exp_sum}{{6.2}{44}}
\newlabel{tab:cfp_exp_sum@cref}{{[table][2][6]6.2}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Re-centralized Quantization}{45}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Parameter distribution of the first fully connected layer in \textit  { LeNet5}, and the color coded arithmetic precisions using \textit  {Customized floating-point quantization} with $1$-bit sign, $1$-bit exponent and $3$-bit fraction. A deeper red color corresponds to a greater arithmetic precision, the quantization is non-linear since the representations is focused on a particular range.}}{46}}
\newlabel{fig:weight_dist_cfc}{{6.2}{46}}
\newlabel{fig:weight_dist_cfc@cref}{{[figure][2][6]6.2}{46}}
\@writefile{toc}{\contentsline {subsubsection}{Method Description}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Parameter distribution of the first fully connected layer in \textit  { LeNet5}, and the color coded arithmetic precisions using \textit  {Centralized customized floating-point quantization} with $1$-bit sign, $1$-bit centre, $1$-bit exponent and $3$-bit fraction. A deeper red color corresponds to a greater arithmetic precision, the quantization is non-linear since the representations is focused on particular ranges.}}{47}}
\newlabel{fig:prune_dist}{{6.3}{47}}
\newlabel{fig:prune_dist@cref}{{[figure][3][6]6.3}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Number representation system for \textit  {Re-centralized customized floating-point quantization} with 1-bit sign, 1-bit central, E-bit exponent and M-bit mantissa.}}{47}}
\newlabel{fig:number_rep_ccfp}{{6.4}{47}}
\newlabel{fig:number_rep_ccfp@cref}{{[figure][4][6]6.4}{47}}
\citation{Han15}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Number representation system for \textit  {Re-centralized dynmaic fixed-point quantization} with 1-bit sign, 1-bit central and $N$-bit fraction.}}{48}}
\newlabel{fig:number_rep_cdfp}{{6.5}{48}}
\newlabel{fig:number_rep_cdfp@cref}{{[figure][5][6]6.5}{48}}
\newlabel{equ:d2dfp}{{6.2}{48}}
\newlabel{equ:d2dfp@cref}{{[equation][2][6]6.2}{48}}
\newlabel{equ:d2dfp}{{6.3}{48}}
\newlabel{equ:d2dfp@cref}{{[equation][3][6]6.3}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Distribution of weights (blue) and distribution of codebook before (green) and after \textit  {Weights sharing}(red).}}{49}}
\newlabel{fig:ws}{{6.6}{49}}
\newlabel{fig:ws@cref}{{[figure][6][6]6.6}{49}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Quantization summary on a pruned \textit  {LeNet5} model. \textit  {Customized floating-point} has 1-bit sign, 1-bit exponent and the rest are mantissa bits. \textit  {Centralized customized floating-point} has 1-bit sign, 1-bit central, 1-bit exponent and the rest are mantissa bits. \textit  {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{50}}
\newlabel{tab:pruned_quan1}{{6.3}{50}}
\newlabel{tab:pruned_quan1@cref}{{[table][3][6]6.3}{50}}
\citation{Gysel}
\citation{Gysel}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Quantization summary on a pruned \textit  {CifarNet} model. \textit  {Customized floating-point} has 1-bit sign, 2-bit exponent and the rest are mantissa bits. \textit  {Centralized customized floating-point} has 1-bit sign, 1-bit central, 2-bit exponent and the rest are mantissa bits. \textit  {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{51}}
\newlabel{tab:pruned_quan2}{{6.4}{51}}
\newlabel{tab:pruned_quan2@cref}{{[table][4][6]6.4}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Summary of Quantization Methods}{51}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Quantization summary (bit-width, error rate) on unpruned \textit  {LeNet5} and \textit  {CifarNet}. \textit  {DFP} is \textit  {Dynamic fixed-point}, \textit  {CFP} is \textit  {Customized floating-point} and (\textit  {Gysel}) is \textit  {Gysel et al.}'s implemenetation of \textit  {Dynamic fixed-point} \cite  {Gysel}.}}{52}}
\newlabel{tab:quantize_summary}{{6.5}{52}}
\newlabel{tab:quantize_summary@cref}{{[table][5][6]6.5}{52}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Quantization summary (bit-width, error rate) on pruned \textit  {LeNet5} and \textit  {CifarNet}. \textit  {CFP} is \textit  {Customized floating-point} \textit  {CCFP} is \textit  {Centralized customized floating-point} and \textit  {CDFP} is \textit  {Centralized dynmaic fixed-point}. }}{52}}
\newlabel{tab:quantize_summary2}{{6.6}{52}}
\newlabel{tab:quantize_summary2@cref}{{[table][6][6]6.6}{52}}
\citation{Han15}
\citation{Han15}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Evaluation}{53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Compression Pipeline}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Overview of \textit  {Deep Compression} \cite  {Han15}.}}{54}}
\newlabel{fig:deep_compression_flow}{{7.1}{54}}
\newlabel{fig:deep_compression_flow@cref}{{[figure][1][7]7.1}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Overview of the proposed compression pipeline.}}{54}}
\newlabel{fig:proposed_compression_flow}{{7.2}{54}}
\newlabel{fig:proposed_compression_flow@cref}{{[figure][2][7]7.2}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Evaluation of Performance}{55}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces \textit  {LeNet5} compression summary, Pruning and Quantization, CR is the compression rate, ER is the error rate. P represnets pruning and Q reporesents quantization.}}{55}}
\newlabel{tab:comp_summary}{{7.1}{55}}
\newlabel{tab:comp_summary@cref}{{[table][1][7]7.1}{55}}
\citation{Tien}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces \textit  {CifarNet} compression summary, Pruning and Quantization, CR is the compression rate, ER is the error rate. P represnets pruning and Q reporesents quantization.}}{56}}
\newlabel{tab:comp_summary2}{{7.2}{56}}
\newlabel{tab:comp_summary2@cref}{{[table][2][7]7.2}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Evaluation of Compression Techniques}{56}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Summary and Conclusions}{58}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Conclusion}{58}}
\citation{Thimm}
\citation{Li2016}
\citation{Kang}
\citation{Kang}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Future Works}{59}}
\citation{tann2017hardware}
\citation{Hassibi}
\citation{DBLP:journals/corr/MolchanovTKAK16}
\citation{Krizhevsky}
\citation{DBLP:journals/corr/SimonyanZ14a}
\citation{szegedy2015going}
\bibstyle{unsrt}
\bibdata{dissertation}
\bibcite{deng2009imagenet}{1}
\bibcite{Krizhevsky}{2}
\bibcite{Guo}{3}
\bibcite{Han15}{4}
\bibcite{Gysel}{5}
\bibcite{Lecun1998gradient}{6}
\bibcite{Szegedy}{7}
\bibcite{Coates}{8}
\bibcite{DBLP:journals/corr/SimonyanZ14a}{9}
\bibcite{DBLP:journals/corr/IoffeS15}{10}
\bibcite{DBLP:journals/corr/SzegedyVISW15}{11}
\bibcite{DBLP:journals/corr/HeZRS15}{12}
\bibcite{Mnih}{13}
\bibcite{Mnih2016}{14}
\bibcite{Tien}{15}
\bibcite{chen2014dadiannao}{16}
\bibcite{chen2014diannao}{17}
\bibcite{han2016eie}{18}
\bibcite{han2016ese}{19}
\bibcite{zhang2015optimizing}{20}
\bibcite{Nurvitadhi:2017:FBG:3020078.3021740}{21}
\bibcite{chen2017eyeriss}{22}
\bibcite{Goodfellow-et-al-2016}{23}
\bibcite{Mladenic}{24}
\bibcite{hecht1988theory}{25}
\bibcite{Hassibi}{26}
\bibcite{Srinivas2015}{27}
\bibcite{mao2017exploring}{28}
\bibcite{DBLP:journals/corr/LiKDSG16}{29}
\bibcite{DBLP:journals/corr/WenWWCL16}{30}
\bibcite{DBLP:journals/corr/LebedevL15}{31}
\bibcite{chen2015compressing}{32}
\bibcite{moons2016energy}{33}
\bibcite{Hubara}{34}
\bibcite{Courbariaux}{35}
\bibcite{li2016ternary}{36}
\bibcite{tensorflow}{37}
\bibcite{lecun1998mnist}{38}
\bibcite{krizhevsky2014cifar}{39}
\bibcite{grother1995nist}{40}
\bibcite{lecun2015lenet}{41}
\bibcite{torralba200880}{42}
\bibcite{krizhevsky2009learning}{43}
\bibcite{jia2014caffe}{44}
\bibcite{Han}{45}
\bibcite{Thimm}{46}
\bibcite{molchanov2016pruning}{47}
\bibcite{nie2010efficient}{48}
\bibcite{mellempudi2017mixed}{49}
\bibcite{kanungo2002efficient}{50}
\bibcite{ercegovac2004digital}{51}
\bibcite{williamson1991dynamically}{52}
\bibcite{courbariaux2014training}{53}
\bibcite{Li2016}{54}
\bibcite{Kang}{55}
\bibcite{tann2017hardware}{56}
\bibcite{DBLP:journals/corr/MolchanovTKAK16}{57}
\bibcite{szegedy2015going}{58}
