\relax 
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Guo}
\citation{Han15}
\citation{Han15}
\citation{Lecun1998gradient}
\citation{Krizhevsky}
\citation{Szegedy}
\citation{Lecun1998gradient}
\citation{Krizhevsky}
\citation{Coates}
\citation{Mnih}
\citation{Mnih2016}
\citation{Han15}
\citation{Tien}
\citation{Tien}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{chen2014dadiannao}
\citation{chen2014diannao}
\citation{han2016eie}
\citation{han2016ese}
\citation{zhang2015optimizing}
\citation{Nurvitadhi:2017:FBG:3020078.3021740}
\citation{han2016eie}
\citation{chen2017eyeriss}
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Mladenic}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Networks}{4}}
\citation{Krizhevsky}
\citation{Krizhevsky}
\citation{Krizhevsky}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces AlexNet Network Architecture \cite  {Krizhevsky}.}}{5}}
\newlabel{fig:alexnet}{{2.1}{5}}
\newlabel{fig:alexnet@cref}{{[figure][1][2]2.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Fully connected layer followed by an activation function \cite  {Krizhevsky}.}}{5}}
\newlabel{fig:neuron}{{2.2}{5}}
\newlabel{fig:neuron@cref}{{[figure][2][2]2.2}{5}}
\citation{Lecun1998gradient}
\citation{Krizhevsky}
\citation{Coates}
\citation{Lecun1998gradient}
\citation{Hassibi}
\citation{Srinivas2015}
\citation{Lecun1998gradient}
\citation{Hassibi}
\citation{Han15}
\citation{Guo}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{6}}
\citation{Han15}
\citation{Han15}
\citation{chen2015compressing}
\citation{moons2016energy}
\citation{Hubara}
\citation{Courbariaux}
\citation{tensorflow}
\citation{lecun1998mnist}
\citation{krizhevsky2014cifar}
\citation{lecun1998mnist}
\citation{lecun2015lenet}
\citation{torralba200880}
\citation{krizhevsky2014cifar}
\citation{krizhevsky2009learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Experiment Setups, Datasets and Trained Networks}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{jia2014caffe}
\citation{Han15}
\citation{Guo}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Number of parameters in LeNet5-431K.}}{10}}
\newlabel{tab:LeNetparam}{{3.1}{10}}
\newlabel{tab:LeNetparam@cref}{{[table][1][3]3.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces LeNet5 Network Architecture}}{10}}
\newlabel{fig:lenet5_arch}{{3.1}{10}}
\newlabel{fig:lenet5_arch@cref}{{[figure][1][3]3.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces CifarNet Network Architecture}}{11}}
\newlabel{fig:Cifarnetparam}{{3.2}{11}}
\newlabel{fig:Cifarnetparam@cref}{{[figure][2][3]3.2}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Number of parameters in CifarNet.}}{11}}
\newlabel{tab:CifarNetparam}{{3.2}{11}}
\newlabel{tab:CifarNetparam@cref}{{[table][2][3]3.2}{11}}
\citation{Han15}
\citation{Guo}
\citation{Thimm}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Pruning}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:dprune}{{4}{12}}
\newlabel{sec:dprune@cref}{{[chapter][4][]4}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Deterministic Pruning}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Pruning with biases}{12}}
\newlabel{prune_without_biase}{{4.1.1}{12}}
\newlabel{prune_without_biase@cref}{{[subsection][1][4,1]4.1.1}{12}}
\citation{Guo}
\newlabel{equ:hfunc}{{4.1}{13}}
\newlabel{equ:hfunc@cref}{{[equation][1][4]4.1}{13}}
\newlabel{equ:tn}{{4.2}{13}}
\newlabel{equ:tn@cref}{{[equation][2][4]4.2}{13}}
\citation{Guo}
\newlabel{equ:minfunc}{{4.3}{14}}
\newlabel{equ:minfunc@cref}{{[equation][3][4]4.3}{14}}
\newlabel{equ:trainfunc}{{4.4}{14}}
\newlabel{equ:trainfunc@cref}{{[equation][4][4]4.4}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Distribution of weights in fc1 layer of pruned and unpruned LeNet5}}{15}}
\newlabel{fig:dist_weights}{{4.1}{15}}
\newlabel{fig:dist_weights@cref}{{[figure][1][4]4.1}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Number of parameters of pruned LeNet5-431K.}}{15}}
\newlabel{tab:LeNetPrune}{{4.1}{15}}
\newlabel{tab:LeNetPrune@cref}{{[table][1][4]4.1}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Number of parameters of pruned \textit  {Cifarnet}.}}{15}}
\newlabel{tab:CifarNetPrune}{{4.2}{15}}
\newlabel{tab:CifarNetPrune@cref}{{[table][2][4]4.2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Pruning without biases}{16}}
\newlabel{equ:lossmin2}{{4.5}{16}}
\newlabel{equ:lossmin2@cref}{{[equation][5][4]4.5}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Number of parameters of pruned LeNet5-431K, without pruning biases.}}{16}}
\newlabel{tab:LeNetPrune2}{{4.3}{16}}
\newlabel{tab:LeNetPrune2@cref}{{[table][3][4]4.3}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Number of parameters of pruned CifarNet, without pruning biases.}}{16}}
\newlabel{tab:CifarNetPrune2}{{4.4}{16}}
\newlabel{tab:CifarNetPrune2@cref}{{[table][4][4]4.4}{16}}
\newlabel{prune_without_biase}{{4.1.2}{16}}
\newlabel{prune_without_biase@cref}{{[subsection][2][4,1]4.1.2}{16}}
\citation{Guo}
\citation{Guo}
\citation{Guo}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dynamic Network Surgery}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Mechanism of Dynamic network surgery \cite  {Guo}.}}{17}}
\newlabel{fig:dynamic_network_surgery}{{4.2}{17}}
\newlabel{fig:dynamic_network_surgery@cref}{{[figure][2][4]4.2}{17}}
\newlabel{equ:hfunc_ds}{{4.6}{18}}
\newlabel{equ:hfunc_ds@cref}{{[equation][6][4]4.6}{18}}
\newlabel{equ:hfunc_ds}{{4.7}{18}}
\newlabel{equ:hfunc_ds@cref}{{[equation][7][4]4.7}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Number of parameters of pruned LeNet5-431K using Dynamic network surgery}}{18}}
\newlabel{tab:LeNetPrune3}{{4.5}{18}}
\newlabel{tab:LeNetPrune3@cref}{{[table][5][4]4.5}{18}}
\citation{Han15}
\citation{Guo}
\citation{Han15}
\citation{Guo}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Number of parameters of pruned CifarNet using Dynamic network surgery.}}{19}}
\newlabel{tab:CifarNetPrune2}{{4.6}{19}}
\newlabel{tab:CifarNetPrune2@cref}{{[table][6][4]4.6}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Comparison to Existing Works}{19}}
\newlabel{sec:pruning_ext_comp}{{4.3}{19}}
\newlabel{sec:pruning_ext_comp@cref}{{[section][3][4]4.3}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces LeNet5 pruning summary, CR is the compression rate, ER is the error rate. (\textit  {Han}) is deterministic pruning used by \textit  {Han et al.}. (\textit  {Guo}) is the original \textit  {Dynamic network surgery} implemented by \textit  {Guo et al.}. (a), (b), (c), (d) are my implementations of various methods. (a) is deterministic pruning with biases, (b) is deterministic pruning without biases, (c) is \textit  {Dynamic network surgery}, (d) is also \textit  {Dynamic network surgery} but with the same error rate as (\textit  {Guo}).}}{19}}
\newlabel{fig:prune_org_summary}{{4.7}{19}}
\newlabel{fig:prune_org_summary@cref}{{[table][7][4]4.7}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces CifarNet pruning summary, CR is the compression rate, ER is the error rate. (a) is deterministic pruning with biases, (b) is deterministic pruning without biases, (c) is \textit  {Dynamic network surgery}.}}{20}}
\newlabel{fig:cifar_prune_org_summary}{{4.8}{20}}
\newlabel{fig:cifar_prune_org_summary@cref}{{[table][8][4]4.8}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Proposed Pruning Strategies}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Gradient Profiling}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Method Description}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \textit  {Gradient Profiling}.}}{23}}
\newlabel{fig:gp_mech}{{5.1}{23}}
\newlabel{fig:gp_mech@cref}{{[figure][1][5]5.1}{23}}
\newlabel{equ:gmfunc}{{5.1}{23}}
\newlabel{equ:gmfunc@cref}{{[equation][1][5]5.1}{23}}
\citation{molchanov2016pruning}
\newlabel{equ:minfuncgp}{{5.2}{24}}
\newlabel{equ:minfuncgp@cref}{{[equation][2][5]5.2}{24}}
\newlabel{equ:trainfuncgp}{{5.3}{24}}
\newlabel{equ:trainfuncgp@cref}{{[equation][3][5]5.3}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Gradient Profiling and Retraining}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textit  {Gradient profiling} and \textit  {Dynamic network surgery} without retraining.}}{25}}
\newlabel{fig:gpvsds_noretrain}{{5.2}{25}}
\newlabel{fig:gpvsds_noretrain@cref}{{[figure][2][5]5.2}{25}}
\citation{nie2010efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textit  {Gradient profiling} and \textit  {Dynamic network surgery} with 10 epochs retraining.}}{26}}
\newlabel{fig:gpvsds_10retrain}{{5.3}{26}}
\newlabel{fig:gpvsds_10retrain@cref}{{[figure][3][5]5.3}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Number of parameters of pruned LeNet5-431K using \textit  {Gradient profiling}}}{26}}
\newlabel{tab:LeNetPrunegp}{{5.1}{26}}
\newlabel{tab:LeNetPrunegp@cref}{{[table][1][5]5.1}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Regularization Aided Pruning}{26}}
\citation{nie2010efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}$l_1$ and $l_2$ Norms}{27}}
\newlabel{equ:norms}{{5.5}{27}}
\newlabel{equ:norms@cref}{{[equation][5][5]5.5}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Effects of $l1$ and $l2$ norms with different hyperparameters.}}{27}}
\newlabel{fig:l1l2norm}{{5.4}{27}}
\newlabel{fig:l1l2norm@cref}{{[figure][4][5]5.4}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Number of parameters of pruned LeNet5-431K, with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-4}$ and $\lambda _2 = 1e^{-7}$.}}{28}}
\newlabel{tab:LeNetprunel1l2}{{5.2}{28}}
\newlabel{tab:LeNetprunel1l2@cref}{{[table][2][5]5.2}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Number of parameters of pruned CifarNet, with $l1$ and $l2$ norms, $\lambda _1 = 1e^{-5}$ and $\lambda _2 = 1e^{-5}$.}}{28}}
\newlabel{tab:CifarNetPrunel1l2}{{5.3}{28}}
\newlabel{tab:CifarNetPrunel1l2@cref}{{[table][3][5]5.3}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Summary of Pruning Methods}{28}}
\newlabel{sec:pruning_sum}{{5.2.2}{28}}
\newlabel{sec:pruning_sum@cref}{{[subsection][2][5,2]5.2.2}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces LeNet5 Pruning Summary, CR is the compression rate, ER is the error rate. (a) is deterministic pruning with biases, (b) is deterministic pruning without biases, (c) is \textit  {Dynamic network surgery}, (d) is \textit  {Gradient Profiling} and (e) is \textit  {Regularization Aided Pruning}.}}{29}}
\newlabel{fig:prune_new_summary}{{5.4}{29}}
\newlabel{fig:prune_new_summary@cref}{{[table][4][5]5.4}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces \textit  {CifarNet} Pruning Summary, CR is the compression rate, ER is the error rate. (a) is deterministic pruning with biases, (b) is deterministic pruning without biases, (c) is \textit  {Dynamic network surgery} and (d) is \textit  {Regularization Aided Pruning}.}}{29}}
\newlabel{fig:cifar_prune_new_summary}{{5.5}{29}}
\newlabel{fig:cifar_prune_new_summary@cref}{{[table][5][5]5.5}{29}}
\citation{mellempudi2017mixed}
\citation{Han15}
\citation{chen2015compressing}
\citation{Han15}
\citation{kanungo2002efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Quantization}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Weights Sharing}{31}}
\citation{Han15}
\citation{Han15}
\newlabel{equ:weightsshare_cr}{{6.1}{32}}
\newlabel{equ:weightsshare_cr@cref}{{[equation][1][6]6.1}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Weights sharing: training (bottom) and inference (top) \cite  {Han15}.}}{32}}
\newlabel{fig:weights_share}{{6.1}{32}}
\newlabel{fig:weights_share@cref}{{[figure][1][6]6.1}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Fixed-point quantization summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{33}}
\newlabel{tab:ws_sum}{{6.1}{33}}
\newlabel{tab:ws_sum@cref}{{[table][1][6]6.1}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Fixed-point Quantization}{33}}
\citation{ercegovac2004digital}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Number representation system for fixed-point quantization with n-bit fraction.}}{34}}
\newlabel{fig:number_rep_4bit}{{6.2}{34}}
\newlabel{fig:number_rep_4bit@cref}{{[figure][2][6]6.2}{34}}
\newlabel{equ:d2fp}{{6.2}{34}}
\newlabel{equ:d2fp@cref}{{[equation][2][6]6.2}{34}}
\citation{williamson1991dynamically}
\citation{courbariaux2014training}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Fixed-point quantization summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{35}}
\newlabel{tab:fp_sum}{{6.2}{35}}
\newlabel{tab:fp_sum@cref}{{[table][2][6]6.2}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Dynamic Fixed-point Quantization}{35}}
\newlabel{sec:dfq}{{6.3}{35}}
\newlabel{sec:dfq@cref}{{[section][3][6]6.3}{35}}
\citation{courbariaux2014training}
\citation{Gysel}
\citation{courbariaux2014training}
\citation{Gysel}
\newlabel{equ:d2ddfp}{{6.3}{36}}
\newlabel{equ:d2ddfp@cref}{{[equation][3][6]6.3}{36}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Scaling Factor Update}}{36}}
\newlabel{alg:scaling_factor}{{1}{36}}
\newlabel{alg:scaling_factor@cref}{{[algorithm][1][]1}{36}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Dynamic fixed-point quantization summary for \textit  {LeNet5} and \textit  {CifarNet}.}}{37}}
\newlabel{tab:dfp_sum}{{6.3}{37}}
\newlabel{tab:dfp_sum@cref}{{[table][3][6]6.3}{37}}
\citation{courbariaux2014training}
\citation{Gysel}
\citation{ercegovac2004digital}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Proposed Quantization Strategies}{38}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Customized Floating-point Quantization}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Number representation system for customized floating-point quantization with e-bit exponent and m-bit mantissa.}}{39}}
\newlabel{number_rep_dfp}{{7.1}{39}}
\newlabel{number_rep_dfp@cref}{{[figure][1][7]7.1}{39}}
\newlabel{equ:d2dfp2}{{7.1}{39}}
\newlabel{equ:d2dfp2@cref}{{[equation][1][7]7.1}{39}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Customized floating-point quantization summary for \textit  {LeNet5} and \textit  {CifarNet}, with 2-bit exponent and various mantissa width.}}{40}}
\newlabel{tab:cfp_sum}{{7.1}{40}}
\newlabel{tab:cfp_sum@cref}{{[table][1][7]7.1}{40}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Customized floating-point quantization summary for \textit  {LeNet5}, with 3-bit mantissa and various exponent width.}}{40}}
\newlabel{tab:cfp_exp_sum}{{7.2}{40}}
\newlabel{tab:cfp_exp_sum@cref}{{[table][2][7]7.2}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Re-centralized Quantization}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Parameter distribution of the first fully connected layer in \textit  { LeNet5}, and the color coded arithmetic precisions using \textit  {Customized floating-point quantization} with $1$ bit sign, $1$ bit exponent and $3$ bits fraction. A deeper red color corresponds to a greater arithmetic precision.}}{41}}
\newlabel{fig:weight_dist_cfc}{{7.2}{41}}
\newlabel{fig:weight_dist_cfc@cref}{{[figure][2][7]7.2}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Parameter distribution of the first fully connected layer in \textit  { LeNet5}, and the color coded arithmetic precisions using \textit  {Centralized customized floating-point quantization} with $1$ bit sign, $1$ bit centre, $1$ bit exponent and $3$ bits fraction. A deeper red color corresponds to a greater arithmetic precision.}}{42}}
\newlabel{fig:prune_dist}{{7.3}{42}}
\newlabel{fig:prune_dist@cref}{{[figure][3][7]7.3}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Method Description}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Number representation system for \textit  {Re-centralized customized floating-point quantization} with E-bit exponent and M-bit mantissa.}}{43}}
\newlabel{fig:number_rep_ccfp}{{7.4}{43}}
\newlabel{fig:number_rep_ccfp@cref}{{[figure][4][7]7.4}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Number representation system for \textit  {Re-centralized dynmaic fixed-point quantization} with N-bit fraction.}}{43}}
\newlabel{fig:number_rep_cdfp}{{7.5}{43}}
\newlabel{fig:number_rep_cdfp@cref}{{[figure][5][7]7.5}{43}}
\newlabel{equ:d2dfp}{{7.2}{44}}
\newlabel{equ:d2dfp@cref}{{[equation][2][7]7.2}{44}}
\newlabel{equ:d2dfp}{{7.3}{44}}
\newlabel{equ:d2dfp@cref}{{[equation][3][7]7.3}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Quantization summary on a pruned \textit  {LeNet5} model. \textit  {Customized floating-point} has 1-bit sign, 1-bit exponent and the rest are mantissa bits. \textit  {Centralized customized floating-point} has 1-bit sign, 1-bit central, 1-bit exponent and the rest are mantissa bits. \textit  {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{44}}
\newlabel{tab:pruned_quan1}{{7.3}{44}}
\newlabel{tab:pruned_quan1@cref}{{[table][3][7]7.3}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Quantization summary on a pruned \textit  {CifarNet} model. \textit  {Customized floating-point} has 1-bit sign, 2-bit exponent and the rest are mantissa bits. \textit  {Centralized customized floating-point} has 1-bit sign, 1-bit central, 2-bit exponent and the rest are mantissa bits. \textit  {Centralized dynamic fixed-point} has 1-bit sign, 1-bit central and the rest are fraction bits.}}{45}}
\newlabel{tab:pruned_quan2}{{7.4}{45}}
\newlabel{tab:pruned_quan2@cref}{{[table][4][7]7.4}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Summary of Quantization Methods}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Quantization summary (bit-width, error rate) on unpruned \textit  {LeNet5} and \textit  {CifarNet}. \textit  {DFP} is \textit  {Dynamic fixed-point}, \textit  {CFP} is \textit  {Customized floating-point} and (\textit  {Gysel}) is \textit  {Gysel et al.}'s implemenetation of \textit  {Dynamic fixed-point}.}}{46}}
\newlabel{tab:quantize_summary}{{7.5}{46}}
\newlabel{tab:quantize_summary@cref}{{[table][5][7]7.5}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {7.6}{\ignorespaces Quantization summary (bit-width, error rate) on pruned \textit  {LeNet5} and \textit  {CifarNet}. \textit  {CFP} is \textit  {Customized floating-point} \textit  {CCFP} is \textit  {Centralized customized floating-point} and \textit  {CDFP} is \textit  {Centralized dynmaic fixed-point}. }}{47}}
\newlabel{tab:quantize_summary2}{{7.6}{47}}
\newlabel{tab:quantize_summary2@cref}{{[table][6][7]7.6}{47}}
\citation{Han15}
\citation{Han15}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Evaluation}{48}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Compression Pipeline}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Overview of \textit  {Deep Compression} \cite  {Han15}.}}{49}}
\newlabel{fig:deep_compression_flow}{{8.1}{49}}
\newlabel{fig:deep_compression_flow@cref}{{[figure][1][8]8.1}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Overview of the proposed compression pipeline.}}{49}}
\newlabel{fig:proposed_compression_flow}{{8.2}{49}}
\newlabel{fig:proposed_compression_flow@cref}{{[figure][2][8]8.2}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Evaluation of Performance}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces LeNet5 Pruning Summary, Pruning and Quantization, CR is the compression rate, ER is the error rate.}}{50}}
\newlabel{tab:comp_summary}{{8.1}{50}}
\newlabel{tab:comp_summary@cref}{{[table][1][8]8.1}{50}}
\citation{Tien}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Evaluation of Compression Techniques}{51}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Summary and Conclusions}{52}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Conclusion}{52}}
\citation{Thimm}
\citation{Li2016}
\citation{Kang}
\citation{Kang}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Future Works}{53}}
\citation{tann2017hardware}
\citation{Hassibi}
\citation{DBLP:journals/corr/MolchanovTKAK16}
\bibstyle{unsrt}
\bibdata{dissertation}
\bibcite{Krizhevsky}{1}
\bibcite{Guo}{2}
\bibcite{Han15}{3}
\bibcite{Lecun1998gradient}{4}
\bibcite{Szegedy}{5}
\bibcite{Coates}{6}
\bibcite{Mnih}{7}
\bibcite{Mnih2016}{8}
\bibcite{Tien}{9}
\bibcite{chen2014dadiannao}{10}
\bibcite{chen2014diannao}{11}
\bibcite{han2016eie}{12}
\bibcite{han2016ese}{13}
\bibcite{zhang2015optimizing}{14}
\bibcite{Nurvitadhi:2017:FBG:3020078.3021740}{15}
\bibcite{chen2017eyeriss}{16}
\bibcite{Mladenic}{17}
\bibcite{Hassibi}{18}
\bibcite{Srinivas2015}{19}
\bibcite{chen2015compressing}{20}
\bibcite{moons2016energy}{21}
\bibcite{Hubara}{22}
\bibcite{Courbariaux}{23}
\bibcite{tensorflow}{24}
\bibcite{lecun1998mnist}{25}
\bibcite{krizhevsky2014cifar}{26}
\bibcite{lecun2015lenet}{27}
\bibcite{torralba200880}{28}
\bibcite{krizhevsky2009learning}{29}
\bibcite{jia2014caffe}{30}
\bibcite{Thimm}{31}
\bibcite{molchanov2016pruning}{32}
\bibcite{nie2010efficient}{33}
\bibcite{mellempudi2017mixed}{34}
\bibcite{kanungo2002efficient}{35}
\bibcite{ercegovac2004digital}{36}
\bibcite{williamson1991dynamically}{37}
\bibcite{courbariaux2014training}{38}
\bibcite{Gysel}{39}
\bibcite{Li2016}{40}
\bibcite{Kang}{41}
\bibcite{tann2017hardware}{42}
\bibcite{DBLP:journals/corr/MolchanovTKAK16}{43}
