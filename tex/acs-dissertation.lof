\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces AlexNet Network Architecture \cite {Krizhevsky}.}}{5}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Fully connected layer followed by an activation function \cite {Krizhevsky}.}}{5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces LeNet5 Network Architecture}}{10}
\contentsline {figure}{\numberline {3.2}{\ignorespaces CifarNet Network Architecture}}{11}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Distribution of weights in fc1 layer of pruned and unpruned LeNet5}}{15}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Mechanism of Dynamic network surgery \cite {Guo}.}}{17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textit {Gradient Profiling}.}}{23}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textit {Gradient profiling} and \textit {Dynamic network surgery} without retraining.}}{25}
\contentsline {figure}{\numberline {5.3}{\ignorespaces \textit {Gradient profiling} and \textit {Dynamic network surgery} with 10 epochs retraining.}}{26}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Effects of $l1$ and $l2$ norms with different hyperparameters.}}{27}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Weights sharing: training (bottom) and inference (top) \cite {Han15}.}}{32}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Number representation system for fixed-point quantization with n-bit fraction.}}{34}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Number representation system for customized floating-point quantization with e-bit exponent and m-bit mantissa.}}{39}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Parameter distribution of the first fully connected layer in \textit { LeNet5}, and the color coded arithmetic precisions using \textit {Customized floating-point quantization} with $1$ bit sign, $1$ bit exponent and $3$ bits fraction. A deeper red color corresponds to a greater arithmetic precision.}}{41}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Parameter distribution of the first fully connected layer in \textit { LeNet5}, and the color coded arithmetic precisions using \textit {Centralized customized floating-point quantization} with $1$ bit sign, $1$ bit centre, $1$ bit exponent and $3$ bits fraction. A deeper red color corresponds to a greater arithmetic precision.}}{42}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Number representation system for \textit {Re-centralized customized floating-point quantization} with E-bit exponent and M-bit mantissa.}}{43}
\contentsline {figure}{\numberline {7.5}{\ignorespaces Number representation system for \textit {Re-centralized dynmaic fixed-point quantization} with N-bit fraction.}}{43}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces Overview of \textit {Deep Compression} \cite {Han15}.}}{49}
\contentsline {figure}{\numberline {8.2}{\ignorespaces Overview of the proposed compression pipeline.}}{49}
\addvspace {10\p@ }
